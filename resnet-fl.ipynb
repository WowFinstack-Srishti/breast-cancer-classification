{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T18:19:26.139829Z",
     "iopub.status.busy": "2025-10-17T18:19:26.139106Z",
     "iopub.status.idle": "2025-10-17T18:19:26.146450Z",
     "shell.execute_reply": "2025-10-17T18:19:26.145709Z",
     "shell.execute_reply.started": "2025-10-17T18:19:26.139800Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü©π Patching 'patch_extraction_and_csv.py' to correctly handle 'InSitu' folder...\n",
      "‚úÖ Script 'patch_extraction_and_csv.py' has been updated with the fix.\n"
     ]
    }
   ],
   "source": [
    "# --- 1.0. FIX THE DATA EXTRACTION SCRIPT ---\n",
    "print(\"ü©π Patching 'patch_extraction_and_csv.py' to correctly handle 'InSitu' folder...\")\n",
    "\n",
    "corrected_script_code = \"\"\"\n",
    "import os, h5py, random, argparse, numpy as np, pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# It's better to try importing stain_norm and handle failure\n",
    "try:\n",
    "    from scripts.stain_norm import normalize_staining\n",
    "except ImportError:\n",
    "    from stain_norm import normalize_staining\n",
    "\n",
    "def ensure_rgb(img):\n",
    "    if isinstance(img, Image.Image): img = np.array(img)\n",
    "    if img.ndim == 2: img = np.stack([img] * 3, axis=-1)\n",
    "    return img\n",
    "\n",
    "def extract_bach(raw_dir, out_dir):\n",
    "    samples = []\n",
    "    # --- START OF THE FIX ---\n",
    "    # Flexible map to handle different folder names for 'in_situ'\n",
    "    label_map = {'normal': 0, 'benign': 1, 'in_situ': 2, 'insitu': 2, 'in-situ': 2, 'invasive': 3}\n",
    "    # --- END OF THE FIX ---\n",
    "    print(f\"Scanning for BACH image folders in: {raw_dir}\")\n",
    "    for cls_folder_name in os.listdir(raw_dir):\n",
    "        d = os.path.join(raw_dir, cls_folder_name)\n",
    "        if os.path.isdir(d):\n",
    "            label = label_map.get(cls_folder_name.lower(), None)\n",
    "            if label is None:\n",
    "                print(f\"--> Skipping unknown folder: {cls_folder_name}\")\n",
    "                continue\n",
    "            print(f\"--> Processing folder: {cls_folder_name} as Label {label}\")\n",
    "            for f in os.listdir(d):\n",
    "                if f.lower().endswith(('.png', '.tif', '.jpg')):\n",
    "                    samples.append((os.path.join(d, f), label))\n",
    "    random.shuffle(samples)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    csv = []\n",
    "    for idx, (fp, label) in enumerate(samples):\n",
    "        try:\n",
    "            img = Image.open(fp).convert('RGB')\n",
    "            w, h = img.size; step = 224; count = 0\n",
    "            for y in range(0, h - step + 1, step):\n",
    "                for x in range(0, w - step + 1, step):\n",
    "                    patch = img.crop((x, y, x + step, y + step))\n",
    "                    patch_np = ensure_rgb(patch)\n",
    "                    if np.mean(patch_np) > 230 and np.std(patch_np) < 15: continue # Skip white background patches\n",
    "                    patch_normalized = normalize_staining(patch_np)\n",
    "                    fn = f'bach_{idx:04d}_{count:03d}.png'\n",
    "                    Image.fromarray(patch_normalized).save(os.path.join(out_dir, fn))\n",
    "                    csv.append({'filename': f'bach/{fn}', 'label': label})\n",
    "                    count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not process file {fp}. Error: {e}\")\n",
    "    return csv\n",
    "\n",
    "def split_and_save(csv_list, processed_csv_dir):\n",
    "    if not csv_list:\n",
    "        print(\"‚ùå No data was processed into the CSV list. Halting.\")\n",
    "        return\n",
    "    df = pd.DataFrame(csv_list)\n",
    "    if 'label' not in df.columns or len(df['label'].unique()) < 2:\n",
    "        print(f\"‚ùå Cannot stratify split with single class or no labels. Found labels: {df['label'].unique()}\")\n",
    "        return\n",
    "    train, rest = train_test_split(df, test_size=0.3, stratify=df['label'], random_state=42)\n",
    "    val, test = train_test_split(rest, test_size=0.5, stratify=rest['label'], random_state=42)\n",
    "    os.makedirs(processed_csv_dir, exist_ok=True)\n",
    "    train.to_csv(os.path.join(processed_csv_dir, 'train.csv'), index=False)\n",
    "    val.to_csv(os.path.join(processed_csv_dir, 'val.csv'), index=False)\n",
    "    test.to_csv(os.path.join(processed_csv_dir, 'test.csv'), index=False)\n",
    "    print(\"‚úÖ CSV splits created at\", processed_csv_dir)\n",
    "\n",
    "def main(args):\n",
    "    all_csv = []\n",
    "    os.makedirs(args.out_patches, exist_ok=True)\n",
    "    if args.bach:\n",
    "        out = os.path.join(args.out_patches, 'bach')\n",
    "        all_csv += extract_bach(args.bach, out)\n",
    "    split_and_save(all_csv, args.csv_dir)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--bach', help='BACH raw folder')\n",
    "    parser.add_argument('--out_patches', default='data/processed/patches')\n",
    "    parser.add_argument('--csv_dir', default='data/processed')\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n",
    "\"\"\"\n",
    "\n",
    "# Write the corrected code to the file\n",
    "with open('scripts/patch_extraction_and_csv.py', 'w') as f:\n",
    "    f.write(corrected_script_code)\n",
    "\n",
    "print(\"‚úÖ Script 'patch_extraction_and_csv.py' has been updated with the fix.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T18:19:34.152087Z",
     "iopub.status.busy": "2025-10-17T18:19:34.151513Z",
     "iopub.status.idle": "2025-10-17T18:29:27.733844Z",
     "shell.execute_reply": "2025-10-17T18:29:27.733092Z",
     "shell.execute_reply.started": "2025-10-17T18:19:34.152063Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è Running patch extraction for the BACH dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(20921) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for BACH image folders in: data/raw/bach/ICIAR2018_BACH_Challenge/Photos/\n",
      "--> Processing folder: InSitu as Label 2\n",
      "--> Processing folder: Invasive as Label 3\n",
      "--> Processing folder: Benign as Label 1\n",
      "--> Processing folder: Normal as Label 0\n",
      "/Users/vishwaraj/Documents/project/breast-cancer-classification/scripts/stain_norm.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  img_norm = Io * np.exp(-C2)\n",
      "/Users/vishwaraj/Documents/project/breast-cancer-classification/scripts/stain_norm.py:61: RuntimeWarning: overflow encountered in multiply\n",
      "  img_norm = Io * np.exp(-C2)\n",
      "‚úÖ CSV splits created at data/processed/csvs\n",
      "\n",
      "‚úÖ Data regeneration complete. New CSVs and patches are ready in data/processed/\n",
      "total 1328\n",
      "-rw-r--r--@ 1 vishwaraj  staff   80565 Oct 18 14:44 test.csv\n",
      "-rw-r--r--@ 1 vishwaraj  staff  375840 Oct 18 14:44 train.csv\n",
      "-rw-r--r--@ 1 vishwaraj  staff   80565 Oct 18 14:44 val.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(21061) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "# --- 1.1. REGENERATE DATASET (BACH Only) ---\n",
    "print(\"üõ†Ô∏è Running patch extraction for the BACH dataset...\")\n",
    "\n",
    "# Path to your local raw BACH photos\n",
    "BACH_RAW_PATH = 'data/raw/bach/ICIAR2018_BACH_Challenge/Photos/'\n",
    "\n",
    "# Paths where the processed data will be saved\n",
    "NEW_PATCHES_DIR = 'data/processed/patches'\n",
    "NEW_CSVS_DIR = 'data/processed/csvs'\n",
    "\n",
    "# Construct and run the command targeting only the BACH dataset\n",
    "# Note the use of the '!' to run a shell command from the notebook\n",
    "!python scripts/patch_extraction_and_csv.py \\\n",
    "    --bach \"{BACH_RAW_PATH}\" \\\n",
    "    --out_patches \"{NEW_PATCHES_DIR}\" \\\n",
    "    --csv_dir \"{NEW_CSVS_DIR}\"\n",
    "\n",
    "print(\"\\n‚úÖ Data regeneration complete. New CSVs and patches are ready in data/processed/\")\n",
    "\n",
    "# Verify that the new train.csv file has been created\n",
    "!ls -l {NEW_CSVS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T13:44:05.647573Z",
     "iopub.status.busy": "2025-10-17T13:44:05.646653Z",
     "iopub.status.idle": "2025-10-17T13:44:05.747613Z",
     "shell.execute_reply": "2025-10-17T13:44:05.746815Z",
     "shell.execute_reply.started": "2025-10-17T13:44:05.647514Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Simulating 3 hospitals with non-IID data from the newly generated dataset...\n",
      "\n",
      "--- Verifying Source CSV Class Distribution ---\n",
      "label\n",
      "0    3743\n",
      "1    3764\n",
      "2    3754\n",
      "3    3772\n",
      "Name: count, dtype: int64\n",
      "---------------------------------------------\n",
      "\n",
      "Saved client 0 data to data/processed/client_csvs/client_0_train.csv\n",
      "Client 0 distribution:\n",
      "label\n",
      "0    2620\n",
      "1    2634\n",
      "2     187\n",
      "3     188\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved client 1 data to data/processed/client_csvs/client_1_train.csv\n",
      "Client 1 distribution:\n",
      "label\n",
      "0     187\n",
      "1     188\n",
      "2    2627\n",
      "3     188\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved client 2 data to data/processed/client_csvs/client_2_train.csv\n",
      "Client 2 distribution:\n",
      "label\n",
      "0     187\n",
      "1     188\n",
      "2     187\n",
      "3    2640\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚úÖ Non-IID client data created successfully with all 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. CREATE NON-IID DATA PARTITIONS (Updated) ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"üî¨ Simulating 3 hospitals with non-IID data from the newly generated dataset...\")\n",
    "\n",
    "# --- KEY CHANGE: Point to the NEWLY CREATED train.csv ---\n",
    "TRAIN_CSV_PATH = 'data/processed/csvs/train.csv'\n",
    "\n",
    "# Load the full, original training dataset\n",
    "try:\n",
    "    full_train_df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "    print(\"\\n--- Verifying Source CSV Class Distribution ---\")\n",
    "    print(full_train_df['label'].value_counts().sort_index())\n",
    "    print(\"---------------------------------------------\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ERROR: '{TRAIN_CSV_PATH}' not found.\")\n",
    "    print(\"Please ensure the previous cell ran successfully and created the file.\")\n",
    "    raise\n",
    "\n",
    "# BACH dataset labels: 0=Normal, 1=Benign, 2=In-situ, 3=Invasive\n",
    "labels = {0: 'Normal', 1: 'Benign', 2: 'In-situ', 3: 'Invasive'}\n",
    "num_clients = 3\n",
    "all_data = []\n",
    "\n",
    "# Create skewed distributions for each client\n",
    "client_0_dist = {0: 0.70, 1: 0.70, 2: 0.05, 3: 0.05} # Specialist in Normal/Benign\n",
    "client_1_dist = {0: 0.05, 1: 0.05, 2: 0.70, 3: 0.05} # Specialist in In-situ\n",
    "client_2_dist = {0: 0.05, 1: 0.05, 2: 0.05, 3: 0.70} # Specialist in Invasive\n",
    "distributions = [client_0_dist, client_1_dist, client_2_dist]\n",
    "\n",
    "# Shuffle the dataframe before splitting\n",
    "full_train_df = full_train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Assign data to clients based on distributions\n",
    "for label_idx, label_name in labels.items():\n",
    "    class_df = full_train_df[full_train_df['label'] == label_idx]\n",
    "    num_samples = len(class_df)\n",
    "    start_idx = 0\n",
    "    dist_proportions = [dist[label_idx] for dist in distributions]\n",
    "\n",
    "    for i in range(num_clients):\n",
    "        end_idx = start_idx + int(np.floor(num_samples * dist_proportions[i]))\n",
    "        client_data_slice = class_df.iloc[start_idx:end_idx].copy()\n",
    "        client_data_slice['client_id'] = i\n",
    "        all_data.append(client_data_slice)\n",
    "        start_idx = end_idx\n",
    "\n",
    "# Concatenate all client data slices\n",
    "partitioned_df = pd.concat(all_data)\n",
    "\n",
    "# Create the output directory inside the project's data folder\n",
    "# This path is now relative to your project root\n",
    "OUTPUT_DIR = 'data/processed/client_csvs'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Save the individual CSV file for each client\n",
    "for i in range(num_clients):\n",
    "    client_df = partitioned_df[partitioned_df['client_id'] == i].drop(columns=['client_id'])\n",
    "    client_df = client_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    output_path = os.path.join(OUTPUT_DIR, f'client_{i}_train.csv')\n",
    "    client_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nSaved client {i} data to {output_path}\")\n",
    "    print(f\"Client {i} distribution:\\n{client_df['label'].value_counts().sort_index()}\")\n",
    "\n",
    "print(\"\\n‚úÖ Non-IID client data created successfully with all 4 classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/train.py\n",
    "\"\"\"\n",
    "Robust, stand-alone training script with:\n",
    "- Checkpointing (last_epoch.pt, epoch_X.pt)\n",
    "- Resuming from last_epoch.pt\n",
    "- Best model saving (best_epoch.pt)\n",
    "- Early Stopping\n",
    "- Metrics history logging (training_history.csv)\n",
    "- Final testing on test set\n",
    "- Plotting of training/validation curves\n",
    "\"\"\"\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import precision_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Model & Data ---\n",
    "# Note: These must be importable, so they need to be in src/\n",
    "# (Assuming they are in src/datasets.py and src/models.py)\n",
    "\n",
    "try:\n",
    "    from src.datasets import PatchDataset\n",
    "    from src.models import ResNet50Fine, ViTModel\n",
    "except ImportError:\n",
    "    print(\"Warning: Could not import from src. Running standalone.\")\n",
    "    # Define dummy classes for environments where src isn't in path\n",
    "    # This can happen in some notebook setups.\n",
    "    \n",
    "    from torch.utils.data import Dataset\n",
    "    from torchvision.models import resnet50, ResNet50_Weights\n",
    "    from PIL import Image\n",
    "\n",
    "    class PatchDataset(Dataset):\n",
    "        def __init__(self, csv_file, img_dir, transform=None):\n",
    "            self.data_frame = pd.read_csv(csv_file)\n",
    "            self.img_dir = img_dir\n",
    "            self.transform = transform\n",
    "        def __len__(self):\n",
    "            return len(self.data_frame)\n",
    "        def __getitem__(self, idx):\n",
    "            img_name, label = self.data_frame.iloc[idx, 0], self.data_frame.iloc[idx, 1]\n",
    "            image = Image.open(os.path.join(self.img_dir, img_name)).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label, img_name\n",
    "\n",
    "    class ResNet50Fine(nn.Module):\n",
    "        def __init__(self, num_classes=4):\n",
    "            super().__init__()\n",
    "            self.backbone = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "            self.backbone.fc = nn.Linear(self.backbone.fc.in_features, num_classes)\n",
    "        def forward(self, x):\n",
    "            return self.backbone.x(x)\n",
    "\n",
    "def load_model(cfg):\n",
    "    model_config = cfg.get('model', {})\n",
    "    model_type = model_config.get('type', 'resnet')\n",
    "    num_classes = model_config.get('num_classes', 2)\n",
    "    \n",
    "    if model_type == 'resnet':\n",
    "        model = ResNet50Fine(num_classes=num_classes)\n",
    "    else:\n",
    "        raise NotImplementedError(\"ViT model loading not implemented.\")\n",
    "    return model\n",
    "\n",
    "# --- Trainer Class ---\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        # 1. Setup Device\n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device(\"mps\")\n",
    "        elif torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "        print(f\"--- [Trainer] Using device: {self.device} ---\")\n",
    "        \n",
    "        # 2. Setup Configs\n",
    "        self.data_cfg = cfg.get('data', {})\n",
    "        self.train_cfg = cfg.get('training', {})\n",
    "        self.outdir = self.train_cfg.get('outdir', 'experiments/default')\n",
    "        os.makedirs(self.outdir, exist_ok=True, mode=0o777)\n",
    "\n",
    "        # 3. Setup Checkpoint Paths\n",
    "        self.last_ckpt_path = os.path.join(self.outdir, 'last_epoch.pt')\n",
    "        self.best_ckpt_path = os.path.join(self.outdir, 'best_epoch.pt')\n",
    "        self.history_csv_path = os.path.join(self.outdir, 'training_history.csv')\n",
    "        self.plot_path = os.path.join(self.outdir, 'training_plot.png')\n",
    "\n",
    "        # 4. Setup DataLoaders\n",
    "        img_size = self.data_cfg.get('img_size', 224)\n",
    "        train_t = T.Compose([T.RandomResizedCrop(img_size), T.RandomHorizontalFlip(), T.ToTensor(), T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])])\n",
    "        val_t = T.Compose([T.Resize((img_size, img_size)), T.ToTensor(), T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])])\n",
    "\n",
    "        # Train Loader\n",
    "        self.train_ds = PatchDataset(self.data_cfg['train_csv'], self.data_cfg['img_dir'], transform=train_t)\n",
    "        self.train_loader = DataLoader(self.train_ds, batch_size=self.train_cfg.get('batch_size', 32), shuffle=True, num_workers=0)\n",
    "        print(f\"‚úÖ Created training loader with {len(self.train_ds)} samples\")\n",
    "\n",
    "        # Validation Loader\n",
    "        self.val_ds = PatchDataset(self.data_cfg['val_csv'], self.data_cfg['img_dir'], transform=val_t)\n",
    "        self.val_loader = DataLoader(self.val_ds, batch_size=self.train_cfg.get('batch_size', 32), shuffle=False, num_workers=0)\n",
    "        print(f\"‚úÖ Created validation loader with {len(self.val_ds)} samples\")\n",
    "        \n",
    "        # Test Loader\n",
    "        self.test_ds = PatchDataset(self.data_cfg['test_csv'], self.data_cfg['img_dir'], transform=val_t)\n",
    "        self.test_loader = DataLoader(self.test_ds, batch_size=self.train_cfg.get('batch_size', 32), shuffle=False, num_workers=0)\n",
    "        print(f\"‚úÖ Created test loader with {len(self.test_ds)} samples\")\n",
    "\n",
    "        # 5. Setup Model, Optimizer, Scheduler\n",
    "        self.model = load_model(cfg)\n",
    "        self.model.to(self.device)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=self.train_cfg.get('lr', 0.0001))\n",
    "        self.total_epochs = self.train_cfg.get('epochs', 20)\n",
    "        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=self.total_epochs)\n",
    "\n",
    "        # 6. Setup State for Resuming & Early Stopping\n",
    "        self.start_epoch = 0\n",
    "        self.best_val_acc = -1.0\n",
    "        self.epochs_no_improve = 0\n",
    "        self.patience = self.train_cfg.get('early_stopping_patience', 10)\n",
    "        self.history = []\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        if os.path.exists(self.last_ckpt_path):\n",
    "            print(f\"üîÑ Resuming training from checkpoint: {self.last_ckpt_path}\")\n",
    "            checkpoint = torch.load(self.last_ckpt_path, map_location=self.device)\n",
    "            self.model.load_state_dict(checkpoint['model_state'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "            self.scheduler.load_state_dict(checkpoint['scheduler_state'])\n",
    "            self.start_epoch = checkpoint['epoch'] + 1\n",
    "            self.best_val_acc = checkpoint.get('best_val_acc', -1.0) # Use .get for backward compatibility\n",
    "            self.epochs_no_improve = checkpoint.get('epochs_no_improve', 0)\n",
    "            print(f\"‚úÖ Resumed from epoch {self.start_epoch}. Best val_acc so far: {self.best_val_acc:.4f}\")\n",
    "        \n",
    "        if os.path.exists(self.history_csv_path):\n",
    "            self.history = pd.read_csv(self.history_csv_path).to_dict('records')\n",
    "\n",
    "    def save_checkpoint(self, epoch):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state': self.model.state_dict(),\n",
    "            'optimizer_state': self.optimizer.state_dict(),\n",
    "            'scheduler_state': self.scheduler.state_dict(),\n",
    "            'best_val_acc': self.best_val_acc,\n",
    "            'epochs_no_improve': self.epochs_no_improve\n",
    "        }\n",
    "        # Save last_epoch.pt (overwritten)\n",
    "        torch.save(checkpoint, self.last_ckpt_path)\n",
    "        \n",
    "        # Save individual epoch file\n",
    "        epoch_save_path = os.path.join(self.outdir, f'epoch_{epoch+1}.pt')\n",
    "        torch.save(checkpoint, epoch_save_path)\n",
    "        # print(f\"Saved checkpoint to {epoch_save_path}\")\n",
    "\n",
    "    def save_history_to_csv(self):\n",
    "        pd.DataFrame(self.history).to_csv(self.history_csv_path, index=False)\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        running_loss, all_labels, all_preds = 0.0, [], []\n",
    "        loop = tqdm(self.train_loader, desc=f'Train E{epoch+1}/{self.total_epochs}', leave=True)\n",
    "        \n",
    "        for imgs, labels, _ in loop:\n",
    "            imgs, labels = imgs.to(self.device), labels.to(self.device)\n",
    "            preds_logits = self.model(imgs)\n",
    "            loss = self.criterion(preds_logits, labels)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, p = preds_logits.max(1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(p.cpu().numpy())\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "        loss = running_loss / len(self.train_loader)\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        prec = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "        return loss, acc, prec\n",
    "\n",
    "    def validate(self, epoch, loader):\n",
    "        self.model.eval()\n",
    "        running_loss, all_labels, all_preds = 0.0, [], []\n",
    "        desc = f'Validate E{epoch+1}/{self.total_epochs}' if loader == self.val_loader else 'Testing'\n",
    "        loop = tqdm(loader, desc=desc, leave=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, labels, _ in loop:\n",
    "                imgs, labels = imgs.to(self.device), labels.to(self.device)\n",
    "                preds_logits = self.model(imgs)\n",
    "                loss = self.criterion(preds_logits, labels)\n",
    "                running_loss += loss.item()\n",
    "                _, p = preds_logits.max(1)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(p.cpu().numpy())\n",
    "                loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "        loss = running_loss / len(loader)\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        prec = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "        return loss, acc, prec\n",
    "\n",
    "    def run_training(self):\n",
    "        print(f\"üöÄ Starting training for {self.total_epochs} epochs...\")\n",
    "        for epoch in range(self.start_epoch, self.total_epochs):\n",
    "            # 1. Train\n",
    "            train_loss, train_acc, train_prec = self.train_epoch(epoch)\n",
    "            \n",
    "            # 2. Validate\n",
    "            val_loss, val_acc, val_prec = self.validate(epoch, self.val_loader)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1} Results: \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "            \n",
    "            # 3. Log History\n",
    "            self.history.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_loss, 'train_acc': train_acc, 'train_prec': train_prec,\n",
    "                'val_loss': val_loss, 'val_acc': val_acc, 'val_prec': val_prec\n",
    "            })\n",
    "            self.save_history_to_csv()\n",
    "\n",
    "            # 4. Step Scheduler\n",
    "            self.scheduler.step()\n",
    "\n",
    "            # 5. Checkpointing & Best Model\n",
    "            self.save_checkpoint(epoch) # Save last_epoch.pt and epoch_X.pt\n",
    "            \n",
    "            if val_acc > self.best_val_acc:\n",
    "                print(f\"üéâ New best validation accuracy: {val_acc:.4f} (was {self.best_val_acc:.4f}). Saving best model...\")\n",
    "                self.best_val_acc = val_acc\n",
    "                self.epochs_no_improve = 0\n",
    "                torch.save(self.model.state_dict(), self.best_ckpt_path) # Save best_epoch.pt\n",
    "            else:\n",
    "                self.epochs_no_improve += 1\n",
    "                print(f\"Validation accuracy did not improve. Patience: {self.epochs_no_improve}/{self.patience}\")\n",
    "\n",
    "            # 6. Early Stopping\n",
    "            if self.epochs_no_improve >= self.patience:\n",
    "                print(f\"üõë Early stopping triggered at epoch {epoch+1} after {self.patience} epochs with no improvement.\")\n",
    "                break\n",
    "        print(\"üèÅ Training finished.\")\n",
    "\n",
    "    def run_testing(self):\n",
    "        print(\"\\n--- Running Final Test ---\")\n",
    "        if not os.path.exists(self.best_ckpt_path):\n",
    "            print(\"‚ùå No 'best_epoch.pt' model found. Testing with last available model.\")\n",
    "            # Fallback to last checkpoint if best was never saved\n",
    "            if os.path.exists(self.last_ckpt_path):\n",
    "                checkpoint = torch.load(self.last_ckpt_path, map_location=self.device)\n",
    "                self.model.load_state_dict(checkpoint['model_state'])\n",
    "            else:\n",
    "                print(\"‚ùå No models found. Cannot run test.\")\n",
    "                return\n",
    "        else:\n",
    "            print(f\"‚úÖ Loading best model from {self.best_ckpt_path} (Val Acc: {self.best_val_acc:.4f})\")\n",
    "            self.model.load_state_dict(torch.load(self.best_ckpt_path, map_location=self.device))\n",
    "        \n",
    "        test_loss, test_acc, test_prec = self.validate(epoch=0, loader=self.test_loader)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*30)\n",
    "        print(\"üéØ FINAL TEST RESULTS üéØ\")\n",
    "        print(f\"     Test Loss: {test_loss:.4f}\")\n",
    "        print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"Test Precision: {test_prec:.4f}\")\n",
    "        print(\"=\"*30)\n",
    "\n",
    "    def plot_history(self):\n",
    "        if not self.history:\n",
    "            print(\"No history to plot.\")\n",
    "            return\n",
    "            \n",
    "        df = pd.DataFrame(self.history)\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        fig.suptitle(f\"Training History: {self.train_cfg.get('experiment_name')}\")\n",
    "\n",
    "        ax1.plot(df['epoch'], df['train_loss'], label='Train Loss')\n",
    "        ax1.plot(df['epoch'], df['val_loss'], label='Validation Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "\n",
    "        ax2.plot(df['epoch'], df['train_acc'], label='Train Accuracy')\n",
    "        ax2.plot(df['epoch'], df['val_acc'], label='Validation Accuracy')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.plot_path)\n",
    "        print(f\"üìà Saved training plot to {self.plot_path}\")\n",
    "        plt.show()\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Centralized Training Script\")\n",
    "    parser.add_argument('--config', type=str, required=True, help='Path to the YAML config file')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Load Config\n",
    "    with open(args.config, 'r') as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(cfg)\n",
    "    \n",
    "    # Load checkpoint if it exists\n",
    "    trainer.load_checkpoint()\n",
    "    \n",
    "    # Run Training\n",
    "    trainer.run_training()\n",
    "    \n",
    "    # Plot History\n",
    "    trainer.plot_history()\n",
    "    \n",
    "    # Run Final Test\n",
    "    trainer.run_testing()\n",
    "\n",
    "print(\"‚úÖ src/train.py has been updated with full training, checkpointing, resuming, and testing logic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T13:44:22.303433Z",
     "iopub.status.busy": "2025-10-17T13:44:22.302801Z",
     "iopub.status.idle": "2025-10-17T14:11:27.959170Z",
     "shell.execute_reply": "2025-10-17T14:11:27.958301Z",
     "shell.execute_reply.started": "2025-10-17T13:44:22.303407Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Centralized Baseline Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(21095) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vishwaraj/Documents/project/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "--- [Trainer] Using device: mps ---\n",
      "‚úÖ Created training loader with 15033 samples\n",
      "‚úÖ Created validation loader with 3222 samples\n",
      "‚úÖ Created test loader with 3222 samples\n",
      "/Users/vishwaraj/Documents/project/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/vishwaraj/Documents/project/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "üöÄ Starting training for 20 epochs...\n",
      "Train E1/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 470/470 [07:46<00:00,  1.01it/s, loss=1.11]\n",
      "Validate E1/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:31<00:00,  3.25it/s, loss=1.45]\n",
      "Epoch 1 Results: Train Loss: 1.2049, Train Acc: 0.4514 | Val Loss: 1.2135, Val Acc: 0.4612\n",
      "üéâ New best validation accuracy: 0.4612 (was -1.0000). Saving best model...\n",
      "Train E2/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 470/470 [08:01<00:00,  1.02s/it, loss=1.06]\n",
      "Validate E2/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:29<00:00,  3.37it/s, loss=1.48]\n",
      "Epoch 2 Results: Train Loss: 1.0828, Train Acc: 0.5281 | Val Loss: 1.2911, Val Acc: 0.4417\n",
      "Validation accuracy did not improve. Patience: 1/10\n",
      "Train E3/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 470/470 [07:24<00:00,  1.06it/s, loss=0.695]\n",
      "Validate E3/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:30<00:00,  3.28it/s, loss=1.4]\n",
      "Epoch 3 Results: Train Loss: 1.0127, Train Acc: 0.5606 | Val Loss: 1.3084, Val Acc: 0.4584\n",
      "Validation accuracy did not improve. Patience: 2/10\n",
      "Train E4/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 470/470 [07:23<00:00,  1.06it/s, loss=1.09]\n",
      "Validate E4/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:30<00:00,  3.32it/s, loss=1.14]\n",
      "Epoch 4 Results: Train Loss: 0.9517, Train Acc: 0.5952 | Val Loss: 1.1874, Val Acc: 0.5050\n",
      "üéâ New best validation accuracy: 0.5050 (was 0.4612). Saving best model...\n",
      "Train E5/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 470/470 [07:41<00:00,  1.02it/s, loss=1.1]\n",
      "Validate E5/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:33<00:00,  3.04it/s, loss=1.14]\n",
      "Epoch 5 Results: Train Loss: 0.9095, Train Acc: 0.6137 | Val Loss: 1.2593, Val Acc: 0.4882\n",
      "Validation accuracy did not improve. Patience: 1/10\n",
      "Train E6/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 470/470 [08:12<00:00,  1.05s/it, loss=0.589]\n",
      "Validate E6/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:31<00:00,  3.20it/s, loss=1.17]\n",
      "Epoch 6 Results: Train Loss: 0.8700, Train Acc: 0.6361 | Val Loss: 1.1869, Val Acc: 0.5177\n",
      "üéâ New best validation accuracy: 0.5177 (was 0.5050). Saving best model...\n",
      "Train E7/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 470/470 [08:01<00:00,  1.02s/it, loss=0.853]\n",
      "Validate E7/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:33<00:00,  2.97it/s, loss=0.685]\n",
      "Epoch 7 Results: Train Loss: 0.8274, Train Acc: 0.6559 | Val Loss: 0.9020, Val Acc: 0.6285\n",
      "üéâ New best validation accuracy: 0.6285 (was 0.5177). Saving best model...\n",
      "Train E8/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 470/470 [07:58<00:00,  1.02s/it, loss=0.733]\n",
      "Validate E8/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:34<00:00,  2.94it/s, loss=1.75]\n",
      "Epoch 8 Results: Train Loss: 0.7866, Train Acc: 0.6723 | Val Loss: 1.5203, Val Acc: 0.4497\n",
      "Validation accuracy did not improve. Patience: 1/10\n",
      "Train E9/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 470/470 [07:53<00:00,  1.01s/it, loss=0.707]\n",
      "Validate E9/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:31<00:00,  3.19it/s, loss=1.45]\n",
      "Epoch 9 Results: Train Loss: 0.7478, Train Acc: 0.6895 | Val Loss: 1.2394, Val Acc: 0.5447\n",
      "Validation accuracy did not improve. Patience: 2/10\n",
      "Train E10/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 470/470 [08:01<00:00,  1.02s/it, loss=0.782]\n",
      "Validate E10/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:31<00:00,  3.17it/s, loss=1.16]\n",
      "Epoch 10 Results: Train Loss: 0.7067, Train Acc: 0.7072 | Val Loss: 1.1388, Val Acc: 0.5866\n",
      "Validation accuracy did not improve. Patience: 3/10\n",
      "Train E11/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 470/470 [07:50<00:00,  1.00s/it, loss=0.791]\n",
      "Validate E11/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:32<00:00,  3.13it/s, loss=1.11]\n",
      "Epoch 11 Results: Train Loss: 0.6640, Train Acc: 0.7337 | Val Loss: 1.1175, Val Acc: 0.6009\n",
      "Validation accuracy did not improve. Patience: 4/10\n",
      "Train E12/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 470/470 [07:51<00:00,  1.00s/it, loss=0.946]\n",
      "Validate E12/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:32<00:00,  3.15it/s, loss=0.933]\n",
      "Epoch 12 Results: Train Loss: 0.6255, Train Acc: 0.7456 | Val Loss: 1.2106, Val Acc: 0.5760\n",
      "Validation accuracy did not improve. Patience: 5/10\n",
      "Train E13/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 470/470 [07:50<00:00,  1.00s/it, loss=0.509]\n",
      "Validate E13/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:31<00:00,  3.17it/s, loss=0.811]\n",
      "Epoch 13 Results: Train Loss: 0.5863, Train Acc: 0.7631 | Val Loss: 1.1559, Val Acc: 0.6037\n",
      "Validation accuracy did not improve. Patience: 6/10\n",
      "Train E14/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 470/470 [07:47<00:00,  1.01it/s, loss=0.704]\n",
      "Validate E14/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:31<00:00,  3.19it/s, loss=0.771]\n",
      "Epoch 14 Results: Train Loss: 0.5522, Train Acc: 0.7765 | Val Loss: 1.1600, Val Acc: 0.6037\n",
      "Validation accuracy did not improve. Patience: 7/10\n",
      "Train E15/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 470/470 [07:45<00:00,  1.01it/s, loss=0.57]\n",
      "Validate E15/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:31<00:00,  3.24it/s, loss=0.647]\n",
      "Epoch 15 Results: Train Loss: 0.5204, Train Acc: 0.7907 | Val Loss: 1.0389, Val Acc: 0.6468\n",
      "üéâ New best validation accuracy: 0.6468 (was 0.6285). Saving best model...\n",
      "Train E16/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 470/470 [07:40<00:00,  1.02it/s, loss=0.532]\n",
      "Validate E16/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:31<00:00,  3.24it/s, loss=0.728]\n",
      "Epoch 16 Results: Train Loss: 0.4880, Train Acc: 0.8020 | Val Loss: 1.1977, Val Acc: 0.6192\n",
      "Validation accuracy did not improve. Patience: 1/10\n",
      "Train E17/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 470/470 [07:43<00:00,  1.01it/s, loss=0.486]\n",
      "Validate E17/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:31<00:00,  3.19it/s, loss=0.563]\n",
      "Epoch 17 Results: Train Loss: 0.4586, Train Acc: 0.8143 | Val Loss: 1.0313, Val Acc: 0.6701\n",
      "üéâ New best validation accuracy: 0.6701 (was 0.6468). Saving best model...\n",
      "Train E18/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 470/470 [07:47<00:00,  1.00it/s, loss=0.314]\n",
      "Validate E18/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:31<00:00,  3.23it/s, loss=0.72]\n",
      "Epoch 18 Results: Train Loss: 0.4454, Train Acc: 0.8212 | Val Loss: 1.1252, Val Acc: 0.6527\n",
      "Validation accuracy did not improve. Patience: 1/10\n",
      "Train E19/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 470/470 [07:54<00:00,  1.01s/it, loss=0.313]\n",
      "Validate E19/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:33<00:00,  3.03it/s, loss=0.699]\n",
      "Epoch 19 Results: Train Loss: 0.4351, Train Acc: 0.8242 | Val Loss: 1.1303, Val Acc: 0.6502\n",
      "Validation accuracy did not improve. Patience: 2/10\n",
      "Train E20/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 470/470 [08:21<00:00,  1.07s/it, loss=0.261]\n",
      "Validate E20/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:33<00:00,  3.01it/s, loss=0.732]\n",
      "Epoch 20 Results: Train Loss: 0.4229, Train Acc: 0.8336 | Val Loss: 1.0809, Val Acc: 0.6614\n",
      "Validation accuracy did not improve. Patience: 3/10\n",
      "üèÅ Training finished.\n",
      "üìà Saved training plot to experiments/centralized_baseline/training_plot.png\n",
      "Figure(1500x500)\n",
      "\n",
      "--- Running Final Test ---\n",
      "‚úÖ Loading best model from experiments/centralized_baseline/best_epoch.pt (Val Acc: 0.6701)\n",
      "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:33<00:00,  3.00it/s, loss=1.35]\n",
      "\n",
      "==============================\n",
      "üéØ FINAL TEST RESULTS üéØ\n",
      "     Test Loss: 1.0668\n",
      "  Test Accuracy: 0.6688\n",
      "Test Precision: 0.6975\n",
      "==============================\n",
      "‚úÖ src/train.py has been updated with full training, checkpointing, resuming, and testing logic.\n",
      "\n",
      "‚úÖ Centralized training finished.\n",
      "\n",
      "--- Training Local-Only Models ---\n",
      "\n",
      "--- Preparing to train Client 0 ---\n",
      "‚úÖ Client 0 data file found. Proceeding with training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(22680) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vishwaraj/Documents/project/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "--- [Trainer] Using device: mps ---\n",
      "‚úÖ Created training loader with 5629 samples\n",
      "‚úÖ Created validation loader with 3222 samples\n",
      "‚úÖ Created test loader with 3222 samples\n",
      "/Users/vishwaraj/Documents/project/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/vishwaraj/Documents/project/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "üöÄ Starting training for 20 epochs...\n",
      "Train E1/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [03:07<00:00,  1.07s/it, loss=0.562]\n",
      "Validate E1/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:33<00:00,  3.00it/s, loss=3.07]\n",
      "Epoch 1 Results: Train Loss: 0.8865, Train Acc: 0.5894 | Val Loss: 2.4192, Val Acc: 0.3361\n",
      "üéâ New best validation accuracy: 0.3361 (was -1.0000). Saving best model...\n",
      "Train E2/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [03:06<00:00,  1.06s/it, loss=0.635]\n",
      "Validate E2/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:33<00:00,  2.99it/s, loss=3]\n",
      "Epoch 2 Results: Train Loss: 0.7965, Train Acc: 0.6467 | Val Loss: 2.1801, Val Acc: 0.3932\n",
      "üéâ New best validation accuracy: 0.3932 (was 0.3361). Saving best model...\n",
      "Train E3/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [03:08<00:00,  1.07s/it, loss=0.866]\n",
      "Validate E3/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:34<00:00,  2.95it/s, loss=3.6]\n",
      "Epoch 3 Results: Train Loss: 0.7494, Train Acc: 0.6701 | Val Loss: 2.3347, Val Acc: 0.3718\n",
      "Validation accuracy did not improve. Patience: 1/10\n",
      "Train E4/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [03:13<00:00,  1.10s/it, loss=0.649]\n",
      "Validate E4/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:35<00:00,  2.84it/s, loss=3.17]\n",
      "Epoch 4 Results: Train Loss: 0.7267, Train Acc: 0.6859 | Val Loss: 2.2688, Val Acc: 0.3650\n",
      "Validation accuracy did not improve. Patience: 2/10\n",
      "Train E5/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [03:18<00:00,  1.13s/it, loss=0.492]\n",
      "Validate E5/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:34<00:00,  2.93it/s, loss=3.49]\n",
      "Epoch 5 Results: Train Loss: 0.6962, Train Acc: 0.6975 | Val Loss: 2.4612, Val Acc: 0.3966\n",
      "üéâ New best validation accuracy: 0.3966 (was 0.3932). Saving best model...\n",
      "Train E6/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [03:12<00:00,  1.10s/it, loss=0.539]\n",
      "Validate E6/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:34<00:00,  2.92it/s, loss=3.12]\n",
      "Epoch 6 Results: Train Loss: 0.6645, Train Acc: 0.7207 | Val Loss: 2.3871, Val Acc: 0.3982\n",
      "üéâ New best validation accuracy: 0.3982 (was 0.3966). Saving best model...\n",
      "Train E7/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [03:13<00:00,  1.10s/it, loss=0.557]\n",
      "Validate E7/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:33<00:00,  2.99it/s, loss=3.44]\n",
      "Epoch 7 Results: Train Loss: 0.6485, Train Acc: 0.7250 | Val Loss: 2.4994, Val Acc: 0.3774\n",
      "Validation accuracy did not improve. Patience: 1/10\n",
      "Train E8/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [03:07<00:00,  1.07s/it, loss=0.414]\n",
      "Validate E8/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:32<00:00,  3.08it/s, loss=2.85]\n",
      "Epoch 8 Results: Train Loss: 0.6045, Train Acc: 0.7435 | Val Loss: 2.0611, Val Acc: 0.4109\n",
      "üéâ New best validation accuracy: 0.4109 (was 0.3982). Saving best model...\n",
      "Train E9/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [03:04<00:00,  1.05s/it, loss=0.588]\n",
      "Validate E9/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:32<00:00,  3.06it/s, loss=3.45]\n",
      "Epoch 9 Results: Train Loss: 0.5678, Train Acc: 0.7605 | Val Loss: 2.3663, Val Acc: 0.4119\n",
      "üéâ New best validation accuracy: 0.4119 (was 0.4109). Saving best model...\n",
      "Train E10/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [03:03<00:00,  1.04s/it, loss=0.434]\n",
      "Validate E10/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:32<00:00,  3.09it/s, loss=3.66]\n",
      "Epoch 10 Results: Train Loss: 0.5423, Train Acc: 0.7818 | Val Loss: 2.6340, Val Acc: 0.3945\n",
      "Validation accuracy did not improve. Patience: 1/10\n",
      "Train E11/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [03:08<00:00,  1.07s/it, loss=0.612]\n",
      "Validate E11/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:35<00:00,  2.88it/s, loss=2.88]\n",
      "Epoch 11 Results: Train Loss: 0.5190, Train Acc: 0.7833 | Val Loss: 2.4516, Val Acc: 0.4125\n",
      "üéâ New best validation accuracy: 0.4125 (was 0.4119). Saving best model...\n",
      "Train E12/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [03:17<00:00,  1.12s/it, loss=0.661]\n",
      "Validate E12/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:32<00:00,  3.08it/s, loss=3.6]\n",
      "Epoch 12 Results: Train Loss: 0.4794, Train Acc: 0.8051 | Val Loss: 2.6333, Val Acc: 0.4246\n",
      "üéâ New best validation accuracy: 0.4246 (was 0.4125). Saving best model...\n",
      "Train E13/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [03:07<00:00,  1.06s/it, loss=0.81]\n",
      "Validate E13/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:33<00:00,  3.05it/s, loss=4.07]\n",
      "Epoch 13 Results: Train Loss: 0.4364, Train Acc: 0.8181 | Val Loss: 2.9360, Val Acc: 0.4336\n",
      "üéâ New best validation accuracy: 0.4336 (was 0.4246). Saving best model...\n",
      "Train E14/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [03:10<00:00,  1.08s/it, loss=0.548]\n",
      "Validate E14/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:34<00:00,  2.94it/s, loss=3.44]\n",
      "Epoch 14 Results: Train Loss: 0.4009, Train Acc: 0.8380 | Val Loss: 2.7622, Val Acc: 0.4292\n",
      "Validation accuracy did not improve. Patience: 1/10\n",
      "Train E15/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [03:16<00:00,  1.12s/it, loss=0.234]\n",
      "Validate E15/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:38<00:00,  2.63it/s, loss=3.44]\n",
      "Epoch 15 Results: Train Loss: 0.3883, Train Acc: 0.8410 | Val Loss: 2.8044, Val Acc: 0.4277\n",
      "Validation accuracy did not improve. Patience: 2/10\n",
      "Train E16/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [03:22<00:00,  1.15s/it, loss=0.569]\n",
      "Validate E16/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:35<00:00,  2.88it/s, loss=4.07]\n",
      "Epoch 16 Results: Train Loss: 0.3482, Train Acc: 0.8613 | Val Loss: 3.0245, Val Acc: 0.4252\n",
      "Validation accuracy did not improve. Patience: 3/10\n",
      "Train E17/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [03:09<00:00,  1.08s/it, loss=0.329]\n",
      "Validate E17/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:33<00:00,  3.01it/s, loss=3.97]\n",
      "Epoch 17 Results: Train Loss: 0.3252, Train Acc: 0.8755 | Val Loss: 2.9353, Val Acc: 0.4286\n",
      "Validation accuracy did not improve. Patience: 4/10\n",
      "Train E18/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [03:14<00:00,  1.11s/it, loss=0.461]\n",
      "Validate E18/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:33<00:00,  3.05it/s, loss=3.73]\n",
      "Epoch 18 Results: Train Loss: 0.3126, Train Acc: 0.8742 | Val Loss: 2.7595, Val Acc: 0.4413\n",
      "üéâ New best validation accuracy: 0.4413 (was 0.4336). Saving best model...\n",
      "Train E19/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [03:09<00:00,  1.08s/it, loss=0.449]\n",
      "Validate E19/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:33<00:00,  3.03it/s, loss=3.86]\n",
      "Epoch 19 Results: Train Loss: 0.2917, Train Acc: 0.8833 | Val Loss: 2.8577, Val Acc: 0.4382\n",
      "Validation accuracy did not improve. Patience: 1/10\n",
      "Train E20/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [03:18<00:00,  1.13s/it, loss=0.143]\n",
      "Validate E20/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:34<00:00,  2.93it/s, loss=4.01]\n",
      "Epoch 20 Results: Train Loss: 0.3002, Train Acc: 0.8813 | Val Loss: 2.9340, Val Acc: 0.4302\n",
      "Validation accuracy did not improve. Patience: 2/10\n",
      "üèÅ Training finished.\n",
      "üìà Saved training plot to experiments/local_only_client_0/training_plot.png\n",
      "Figure(1500x500)\n",
      "\n",
      "--- Running Final Test ---\n",
      "‚úÖ Loading best model from experiments/local_only_client_0/best_epoch.pt (Val Acc: 0.4413)\n",
      "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:34<00:00,  2.90it/s, loss=2.33]\n",
      "\n",
      "==============================\n",
      "üéØ FINAL TEST RESULTS üéØ\n",
      "     Test Loss: 2.7863\n",
      "  Test Accuracy: 0.4407\n",
      "Test Precision: 0.5924\n",
      "==============================\n",
      "‚úÖ src/train.py has been updated with full training, checkpointing, resuming, and testing logic.\n",
      "\n",
      "--- Preparing to train Client 1 ---\n",
      "‚úÖ Client 1 data file found. Proceeding with training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(23413) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vishwaraj/Documents/project/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "--- [Trainer] Using device: mps ---\n",
      "‚úÖ Created training loader with 3190 samples\n",
      "‚úÖ Created validation loader with 3222 samples\n",
      "‚úÖ Created test loader with 3222 samples\n",
      "/Users/vishwaraj/Documents/project/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/vishwaraj/Documents/project/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "üöÄ Starting training for 20 epochs...\n",
      "Train E1/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:51<00:00,  1.11s/it, loss=0.485]\n",
      "Validate E1/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:36<00:00,  2.76it/s, loss=1.89]\n",
      "Epoch 1 Results: Train Loss: 0.6702, Train Acc: 0.8150 | Val Loss: 1.9614, Val Acc: 0.3163\n",
      "üéâ New best validation accuracy: 0.3163 (was -1.0000). Saving best model...\n",
      "Train E2/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:53<00:00,  1.13s/it, loss=0.566]\n",
      "Validate E2/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:38<00:00,  2.65it/s, loss=1.98]\n",
      "Epoch 2 Results: Train Loss: 0.5996, Train Acc: 0.8245 | Val Loss: 1.8793, Val Acc: 0.2862\n",
      "Validation accuracy did not improve. Patience: 1/10\n",
      "Train E3/20:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 93/100 [01:39<00:07,  1.06s/it, loss=0.457]"
     ]
    }
   ],
   "source": [
    "# --- 3.B: RUN TRAINING WITH FILE VERIFICATION (UPDATED) ---\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "# --- ‚öôÔ∏è CONFIGURATION YOU CAN CHANGE ---\n",
    "# Increased epochs to 20 to allow early stopping to work\n",
    "EPOCHS_PER_RUN = 20 \n",
    "EARLY_STOPPING_PATIENCE = 10 # Stop after 10 epochs of no improvement\n",
    "# ------------------------------------\n",
    "\n",
    "# Define LOCAL paths to all data locations\n",
    "REGENERATED_DATA_DIR = 'data/processed'\n",
    "CLIENT_CSVS_DIR = 'data/processed/client_csvs' # Directory where client CSVs are saved\n",
    "EXPERIMENTS_DIR = 'experiments' # Top-level directory for all model outputs\n",
    "\n",
    "# This function creates a YAML config file for a training run\n",
    "def create_config(train_csv_path, experiment_name, epochs):\n",
    "    config_path = f'configs/temp_{experiment_name}.yaml'\n",
    "    # Ensure the parent directory for the config exists\n",
    "    os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
    "    \n",
    "    config = {\n",
    "        'data': {\n",
    "            'train_csv': train_csv_path,\n",
    "            'val_csv': os.path.join(REGENERATED_DATA_DIR, 'csvs/val.csv'),\n",
    "            'test_csv': os.path.join(REGENERATED_DATA_DIR, 'csvs/test.csv'),\n",
    "            'img_dir': os.path.join(REGENERATED_DATA_DIR, 'patches'),\n",
    "            'img_size': 224\n",
    "        },\n",
    "        'model': {'num_classes': 4, 'type': 'resnet'},\n",
    "        'training': {\n",
    "            'experiment_name': experiment_name,\n",
    "            'outdir': os.path.join(EXPERIMENTS_DIR, experiment_name),\n",
    "            'epochs': epochs,\n",
    "            'early_stopping_patience': EARLY_STOPPING_PATIENCE, # <-- NEW\n",
    "            'batch_size': 32, 'lr': 0.0001, 'weight_decay': 0.0001, 'use_xai_reg': False\n",
    "        }\n",
    "    }\n",
    "    with open(config_path, 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "    return config_path\n",
    "\n",
    "# --- A. Train Centralized Model ---\n",
    "# This script will now automatically resume if 'last_epoch.pt' is found\n",
    "print(\"--- Training Centralized Baseline Model ---\")\n",
    "central_config = create_config(\n",
    "    train_csv_path=os.path.join(REGENERATED_DATA_DIR, 'csvs/train.csv'),\n",
    "    experiment_name='centralized_baseline',\n",
    "    epochs=EPOCHS_PER_RUN\n",
    ")\n",
    "!python -m src.train --config {central_config}\n",
    "print(\"\\n‚úÖ Centralized training finished.\")\n",
    "\n",
    "\n",
    "# --- B. Train Local-Only Models with Verification ---\n",
    "print(\"\\n--- Training Local-Only Models ---\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Preparing to train Client {i} ---\")\n",
    "    \n",
    "    # Define the full path to the client's training data\n",
    "    local_train_csv_path = os.path.join(CLIENT_CSVS_DIR, f'client_{i}_train.csv')\n",
    "\n",
    "    # --- KEY FIX: VERIFY THE FILE EXISTS BEFORE TRAINING ---\n",
    "    if not os.path.exists(local_train_csv_path):\n",
    "        print(f\"‚ùå CRITICAL ERROR: Client {i}'s data file not found at '{local_train_csv_path}'\")\n",
    "        print(\"--> Please re-run the data partitioning cell to create the client CSV files.\")\n",
    "        break # Stop the loop if a file is missing\n",
    "    else:\n",
    "        print(f\"‚úÖ Client {i} data file found. Proceeding with training...\")\n",
    "        local_config = create_config(\n",
    "            train_csv_path=local_train_csv_path,\n",
    "            experiment_name=f'local_only_client_{i}',\n",
    "            epochs=EPOCHS_PER_RUN\n",
    "        )\n",
    "        !python -m src.train --config {local_config}\n",
    "\n",
    "print(\"\\n‚úÖ All baseline training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-17T18:15:48.776Z",
     "iopub.execute_input": "2025-10-17T16:19:18.689857Z",
     "iopub.status.busy": "2025-10-17T16:19:18.689309Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vishwaraj/Documents/project/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/vishwaraj/Documents/project/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "2025-10-18 13:36:17,667\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.\n",
      "\tInstead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:\n",
      "\n",
      "\t\t$ flwr new  # Create a new Flower app from a template\n",
      "\n",
      "\t\t$ flwr run  # Run the Flower app in Simulation Mode\n",
      "\n",
      "\tUsing `start_simulation()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "\u001b[92mINFO \u001b[0m:      Starting Flower simulation, config: num_rounds=5, no round_timeout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting federated simulation for 5 rounds with 3 clients...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-18 13:36:24,923\tINFO worker.py:1771 -- Started a local Ray instance.\n",
      "\u001b[92mINFO \u001b[0m:      Flower VCE: Ray initialized with resources: {'CPU': 10.0, 'object_store_memory': 2147483648.0, 'node:127.0.0.1': 1.0, 'node:__internal_head__': 1.0, 'memory': 8180747469.0}\n",
      "\u001b[92mINFO \u001b[0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html\n",
      "\u001b[92mINFO \u001b[0m:      No `client_resources` specified. Using minimal resources for clients.\n",
      "\u001b[92mINFO \u001b[0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}\n",
      "\u001b[92mINFO \u001b[0m:      Flower VCE: Creating VirtualClientEngineActorPool with 10 actors\n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n",
      "\u001b[33m(raylet)\u001b[0m /Users/vishwaraj/Documents/project/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m --- [FL Cell Trainer, Client=2] Using device: mps ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m             This is a deprecated feature. It will be removed\n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m             entirely in future versions of Flower.\n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m         \n",
      "\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [FL Cell Trainer, Client=Server_Eval] Using device: mps ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      initial parameters (loss, other metrics): 1.418816357555956, {'accuracy': 0.2585350713842334, 'precision': 0.18921130691095128}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Round 0 Global Model Validation -> Loss: 1.4188, Acc: 0.2585, Prec: 0.1892\n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m --- [FL Cell Trainer, Client=2] Using device: mps ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m             This is a deprecated feature. It will be removed\n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m             entirely in future versions of Flower.\n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m         \n",
      "\u001b[33m(raylet)\u001b[0m /Users/vishwaraj/Documents/project/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\u001b[32m [repeated 9x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m             This is a deprecated feature. It will be removed\n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m             entirely in future versions of Flower.\n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m --- [FL Cell Trainer, Client=0] Using device: mps ---\n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m üìà Client 2 Round 1 -> Loss: 0.6364, Acc: 0.8217, Prec: 0.7375\n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m --- [FL Cell Trainer, Client=1] Using device: mps ---\n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m üìà Client 1 Round 1 -> Loss: 0.7067, Acc: 0.7950, Prec: 0.7019\n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m üìà Client 0 Round 1 -> Loss: 0.8868, Acc: 0.5957, Prec: 0.5650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 3 results and 0 failures\n",
      "\u001b[93mWARNING \u001b[0m:   No fit_metrics_aggregation_fn provided\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Round 1 Aggregated Client Training -> Avg Loss: 0.7723, Avg Acc: 0.7088, Avg Prec: 0.6473\n",
      "--- [FL Cell Trainer, Client=Server_Eval] Using device: mps ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      fit progress: (1, 1.3248985167777185, {'accuracy': 0.351024208566108, 'precision': 0.5338153245555366}, 712.059426625)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Round 1 Global Model Validation -> Loss: 1.3249, Acc: 0.3510, Prec: 0.5338\n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m --- [FL Cell Trainer, Client=0] Using device: mps ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m         \n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 3)\n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m --- [FL Cell Trainer, Client=2] Using device: mps ---\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m üìà Client 1 Round 1 -> Loss: 0.6626, Acc: 0.8006, Prec: 0.7357\n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m --- [FL Cell Trainer, Client=0] Using device: mps ---\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m üìà Client 2 Round 1 -> Loss: 0.6453, Acc: 0.8032, Prec: 0.7101\n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m üìà Client 0 Round 1 -> Loss: 0.8336, Acc: 0.6289, Prec: 0.6082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 3 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Round 2 Aggregated Client Training -> Avg Loss: 0.7381, Avg Acc: 0.7209, Avg Prec: 0.6692\n",
      "--- [FL Cell Trainer, Client=Server_Eval] Using device: mps ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      fit progress: (2, 1.244172784361509, {'accuracy': 0.41247672253258844, 'precision': 0.5099534270933234}, 1402.656449833)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Round 2 Global Model Validation -> Loss: 1.2442, Acc: 0.4125, Prec: 0.5100\n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m --- [FL Cell Trainer, Client=2] Using device: mps ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m         \n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 3)\n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m --- [FL Cell Trainer, Client=2] Using device: mps ---\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m üìà Client 1 Round 1 -> Loss: 0.6365, Acc: 0.8113, Prec: 0.7348\n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m --- [FL Cell Trainer, Client=1] Using device: mps ---\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m üìà Client 2 Round 1 -> Loss: 0.6105, Acc: 0.8123, Prec: 0.7238\n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m üìà Client 0 Round 1 -> Loss: 0.8104, Acc: 0.6458, Prec: 0.6289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 3 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Round 3 Aggregated Client Training -> Avg Loss: 0.7110, Avg Acc: 0.7340, Avg Prec: 0.6823\n",
      "--- [FL Cell Trainer, Client=Server_Eval] Using device: mps ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      fit progress: (3, 1.285461155494841, {'accuracy': 0.39695841092489137, 'precision': 0.5943885265077757}, 2125.4366949999994)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Round 3 Global Model Validation -> Loss: 1.2855, Acc: 0.3970, Prec: 0.5944\n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m --- [FL Cell Trainer, Client=2] Using device: mps ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m         \n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 4]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 3)\n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m üìà Client 1 Round 1 -> Loss: 0.6207, Acc: 0.8172, Prec: 0.7594\n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m --- [FL Cell Trainer, Client=1] Using device: mps ---\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m üìà Client 2 Round 1 -> Loss: 0.6301, Acc: 0.8079, Prec: 0.7183\n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m üìà Client 0 Round 1 -> Loss: 0.7857, Acc: 0.6630, Prec: 0.6434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 3 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Round 4 Aggregated Client Training -> Avg Loss: 0.7005, Avg Acc: 0.7425, Avg Prec: 0.6941\n",
      "--- [FL Cell Trainer, Client=Server_Eval] Using device: mps ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      fit progress: (4, 1.23148776280998, {'accuracy': 0.4075108628181254, 'precision': 0.5697599361844891}, 2737.8092214159997)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Round 4 Global Model Validation -> Loss: 1.2315, Acc: 0.4075, Prec: 0.5698\n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m --- [FL Cell Trainer, Client=2] Using device: mps ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m         \n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 3 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 5]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 3)\n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m --- [FL Cell Trainer, Client=2] Using device: mps ---\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=19422)\u001b[0m üìà Client 1 Round 1 -> Loss: 0.6069, Acc: 0.8125, Prec: 0.7451\n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m --- [FL Cell Trainer, Client=0] Using device: mps ---\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=19423)\u001b[0m üìà Client 2 Round 1 -> Loss: 0.5935, Acc: 0.8154, Prec: 0.7368\n",
      "\u001b[36m(ClientAppActor pid=19421)\u001b[0m üìà Client 0 Round 1 -> Loss: 0.7682, Acc: 0.6697, Prec: 0.6523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(20558) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 211\u001b[0m\n\u001b[1;32m    208\u001b[0m NUM_CLIENTS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müöÄ Starting federated simulation for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_ROUNDS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rounds with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_CLIENTS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m clients...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 211\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mfl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_simulation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_clients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_CLIENTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mServerConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_ROUNDS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müèÅ Federated simulation finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# --- Plot & Save ---\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/project/.venv/lib/python3.9/site-packages/flwr/simulation/legacy_app.py:361\u001b[0m, in \u001b[0;36mstart_simulation\u001b[0;34m(client_fn, num_clients, clients_ids, client_resources, server, config, strategy, client_manager, ray_init_args, keep_initialised, actor_type, actor_kwargs, actor_scheduling)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     hist \u001b[38;5;241m=\u001b[39m \u001b[43mrun_fl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitialized_server\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitialized_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    366\u001b[0m     log(ERROR, ex)\n",
      "File \u001b[0;32m~/Documents/project/.venv/lib/python3.9/site-packages/flwr/server/server.py:492\u001b[0m, in \u001b[0;36mrun_fl\u001b[0;34m(server, config)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_fl\u001b[39m(\n\u001b[1;32m    488\u001b[0m     server: Server,\n\u001b[1;32m    489\u001b[0m     config: ServerConfig,\n\u001b[1;32m    490\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m History:\n\u001b[1;32m    491\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train a model on the given server and return the History object.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 492\u001b[0m     hist, elapsed_time \u001b[38;5;241m=\u001b[39m \u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mround_timeout\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m     log(INFO, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    497\u001b[0m     log(INFO, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SUMMARY]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/project/.venv/lib/python3.9/site-packages/flwr/server/server.py:115\u001b[0m, in \u001b[0;36mServer.fit\u001b[0;34m(self, num_rounds, timeout)\u001b[0m\n\u001b[1;32m    113\u001b[0m log(INFO, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[ROUND \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m, current_round)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Train model and replace previous global model\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m res_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_round\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_round\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res_fit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     parameters_prime, fit_metrics, _ \u001b[38;5;241m=\u001b[39m res_fit  \u001b[38;5;66;03m# fit_metrics_aggregated\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/project/.venv/lib/python3.9/site-packages/flwr/server/server.py:234\u001b[0m, in \u001b[0;36mServer.fit_round\u001b[0;34m(self, server_round, timeout)\u001b[0m\n\u001b[1;32m    226\u001b[0m log(\n\u001b[1;32m    227\u001b[0m     INFO,\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigure_fit: strategy sampled \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m clients (out of \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mlen\u001b[39m(client_instructions),\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_manager\u001b[38;5;241m.\u001b[39mnum_available(),\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# Collect `fit` results from all clients participating in this round\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m results, failures \u001b[38;5;241m=\u001b[39m \u001b[43mfit_clients\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_instructions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_instructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_round\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m log(\n\u001b[1;32m    241\u001b[0m     INFO,\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maggregate_fit: received \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m results and \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m failures\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28mlen\u001b[39m(results),\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28mlen\u001b[39m(failures),\n\u001b[1;32m    245\u001b[0m )\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# Aggregate training results\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/project/.venv/lib/python3.9/site-packages/flwr/server/server.py:353\u001b[0m, in \u001b[0;36mfit_clients\u001b[0;34m(client_instructions, max_workers, timeout, group_id)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m    349\u001b[0m     submitted_fs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    350\u001b[0m         executor\u001b[38;5;241m.\u001b[39msubmit(fit_client, client_proxy, ins, timeout, group_id)\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m client_proxy, ins \u001b[38;5;129;01min\u001b[39;00m client_instructions\n\u001b[1;32m    352\u001b[0m     }\n\u001b[0;32m--> 353\u001b[0m     finished_fs, _ \u001b[38;5;241m=\u001b[39m \u001b[43mconcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubmitted_fs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Handled in the respective communication stack\u001b[39;49;00m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# Gather results\u001b[39;00m\n\u001b[1;32m    359\u001b[0m results: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[ClientProxy, FitRes]] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:306\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(fs, timeout, return_when)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m DoneAndNotDoneFutures(done, not_done)\n\u001b[1;32m    304\u001b[0m     waiter \u001b[38;5;241m=\u001b[39m _create_and_install_waiters(fs, return_when)\n\u001b[0;32m--> 306\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fs:\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m f\u001b[38;5;241m.\u001b[39m_condition:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py:574\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    572\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 574\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- 5. FEDERATED LEARNING (UPDATED with Checkpointing, Resuming, Best Model, and Testing) ---\n",
    "import flwr as fl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from sklearn.metrics import precision_score, accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# --- Define Paths and Configuration ---\n",
    "PROJECT_ROOT = '.'\n",
    "REGENERATED_DATA_PATH = os.path.join(PROJECT_ROOT, 'data/processed')\n",
    "CLIENT_CSVS_PATH = os.path.join(REGENERATED_DATA_PATH, 'client_csvs')\n",
    "EXPERIMENTS_PATH = os.path.join(PROJECT_ROOT, 'experiments')\n",
    "FL_EXPERIMENT_NAME = 'federated_run_final'\n",
    "FL_OUTDIR = os.path.join(EXPERIMENTS_PATH, FL_EXPERIMENT_NAME)\n",
    "FL_CHECKPOINT_DIR = os.path.join(FL_OUTDIR, 'checkpoints')\n",
    "os.makedirs(FL_CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Define Dataset Class ---\n",
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, label = self.data_frame.iloc[idx, 0], self.data_frame.iloc[idx, 1]\n",
    "        image = Image.open(os.path.join(self.img_dir, img_name)).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label, img_name\n",
    "\n",
    "# --- Define Model ---\n",
    "class ResNet50Fine(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# --- Define Helper & Trainer ---\n",
    "def load_model(cfg):\n",
    "    model_config = cfg.get('model', {})\n",
    "    if model_config.get('type', 'resnet') == 'resnet':\n",
    "        return ResNet50Fine(num_classes=model_config.get('num_classes', 4))\n",
    "    raise NotImplementedError(\"Only ResNet is supported.\")\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, cfg, client_id=None):\n",
    "        self.cfg = cfg\n",
    "        self.client_id = client_id\n",
    "        \n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device(\"mps\")\n",
    "        elif torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "        \n",
    "        if client_id is not None:\n",
    "            print(f\"--- [FL Cell Trainer, Client={client_id}] Using device: {self.device} ---\")\n",
    "        \n",
    "        data_cfg, train_cfg = cfg.get('data', {}), cfg.get('training', {})\n",
    "        if train_cfg.get('outdir'): os.makedirs(train_cfg['outdir'], exist_ok=True)\n",
    "        \n",
    "        img_size = data_cfg.get('img_size', 224)\n",
    "        train_t = T.Compose([T.RandomResizedCrop(img_size), T.RandomHorizontalFlip(), T.ToTensor(), T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])])\n",
    "        val_t = T.Compose([T.Resize((img_size, img_size)), T.ToTensor(), T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])])\n",
    "        \n",
    "        NUM_WORKERS = 0 \n",
    "\n",
    "        self.train_loader = None\n",
    "        if data_cfg.get('train_csv') and os.path.exists(data_cfg['train_csv']):\n",
    "            self.train_ds = PatchDataset(data_cfg['train_csv'], data_cfg['img_dir'], transform=train_t)\n",
    "            self.train_loader = DataLoader(self.train_ds, batch_size=train_cfg.get('batch_size', 32), shuffle=True, num_workers=NUM_WORKERS)\n",
    "        \n",
    "        self.val_loader = None\n",
    "        if data_cfg.get('val_csv') and os.path.exists(data_cfg['val_csv']):\n",
    "            self.val_ds = PatchDataset(data_cfg['val_csv'], data_cfg['img_dir'], transform=val_t)\n",
    "            self.val_loader = DataLoader(self.val_ds, batch_size=train_cfg.get('batch_size', 32), shuffle=False, num_workers=NUM_WORKERS)\n",
    "            \n",
    "        self.test_loader = None\n",
    "        if data_cfg.get('test_csv') and os.path.exists(data_cfg['test_csv']):\n",
    "            self.test_ds = PatchDataset(data_cfg['test_csv'], data_cfg['img_dir'], transform=val_t)\n",
    "            self.test_loader = DataLoader(self.test_ds, batch_size=train_cfg.get('batch_size', 32), shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "        self.model = load_model(cfg)\n",
    "        self.model.to(self.device)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=train_cfg.get('lr', 0.0001))\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        if not self.train_loader: return 0.0, 0.0, 0.0\n",
    "        self.model.train()\n",
    "        running_loss, all_labels, all_preds = 0.0, [], []\n",
    "        for imgs, labels, _ in self.train_loader:\n",
    "            imgs, labels = imgs.to(self.device), labels.to(self.device)\n",
    "            preds_logits = self.model(imgs)\n",
    "            loss = self.criterion(preds_logits, labels)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, p = preds_logits.max(1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(p.cpu().numpy())\n",
    "        \n",
    "        return running_loss / len(self.train_loader), accuracy_score(all_labels, all_preds), precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    def validate(self, loader):\n",
    "        if not loader: return 0.0, 0.0, 0.0\n",
    "        self.model.eval()\n",
    "        running_loss, all_labels, all_preds = 0.0, [], []\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels, _ in loader:\n",
    "                imgs, labels = imgs.to(self.device), labels.to(self.device)\n",
    "                preds_logits = self.model(imgs)\n",
    "                running_loss += self.criterion(preds_logits, labels).item()\n",
    "                _, p = preds_logits.max(1)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(p.cpu().numpy())\n",
    "        \n",
    "        loss = running_loss / len(loader)\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        prec = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "        return loss, acc, prec, all_labels, all_preds\n",
    "\n",
    "# --- Define Flower Components ---\n",
    "def get_evaluate_fn(server_config_for_eval):\n",
    "    if not os.path.exists(server_config_for_eval['data']['val_csv']): return None\n",
    "    def evaluate(server_round, parameters, config):\n",
    "        temp_trainer = Trainer(server_config_for_eval, client_id='Server_Eval')\n",
    "        \n",
    "        if torch.backends.mps.is_available(): device = torch.device(\"mps\")\n",
    "        elif torch.cuda.is_available(): device = torch.device(\"cuda\")\n",
    "        else: device = torch.device(\"cpu\")\n",
    "        \n",
    "        params_dict = zip(temp_trainer.model.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.tensor(v).to(device) for k, v in params_dict})\n",
    "        temp_trainer.model.load_state_dict(state_dict)\n",
    "        loss, accuracy, precision, _, _ = temp_trainer.validate(temp_trainer.val_loader)\n",
    "        print(f\"‚úÖ Round {server_round} Global Model Validation -> Loss: {loss:.4f}, Acc: {accuracy:.4f}, Prec: {precision:.4f}\")\n",
    "        return loss, {\"accuracy\": accuracy, \"precision\": precision}\n",
    "    return evaluate\n",
    "\n",
    "# --- NEW: Strategy with Checkpointing, Resuming, and Best Model Saving ---\n",
    "class FedAvgWithCheckpointing(fl.server.strategy.FedAvg):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.checkpoint_dir = kwargs.pop(\"checkpoint_dir\", \"experiments/fl_checkpoints\")\n",
    "        self.last_round_path = os.path.join(self.checkpoint_dir, 'last_round.npz')\n",
    "        \n",
    "        self.best_val_acc = -1.0\n",
    "        self.best_model_params = None\n",
    "        \n",
    "        initial_parameters = None\n",
    "        if os.path.exists(self.last_round_path):\n",
    "            try:\n",
    "                print(f\"üîÑ Resuming from checkpoint: {self.last_round_path}\")\n",
    "                loaded_params = np.load(self.last_round_path)\n",
    "                initial_parameters = fl.common.Parameters(\n",
    "                    tensors=[loaded_params[key] for key in loaded_params.files],\n",
    "                    tensor_type=\"numpy.ndarray\"\n",
    "                )\n",
    "                print(\"‚úÖ Resumed FL simulation from last saved round.\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to load checkpoint: {e}. Starting from round 1.\")\n",
    "\n",
    "        super().__init__(*args, initial_parameters=initial_parameters, **kwargs)\n",
    "\n",
    "    def aggregate_fit(self, server_round, results, failures):\n",
    "        # Aggregate parameters as usual\n",
    "        aggregated_parameters, aggregated_metrics = super().aggregate_fit(server_round, results, failures)\n",
    "        \n",
    "        if aggregated_parameters is not None:\n",
    "            # Save checkpoints\n",
    "            print(f\"üíæ Saving checkpoint for round {server_round}...\")\n",
    "            # Convert Parameters object to list of numpy arrays\n",
    "            params_np = fl.common.parameters_to_ndarrays(aggregated_parameters)\n",
    "            \n",
    "            # Save round-specific checkpoint\n",
    "            np.savez(os.path.join(self.checkpoint_dir, f'round_{server_round}.npz'), *params_np)\n",
    "            # Save (overwrite) last_round checkpoint\n",
    "            np.savez(self.last_round_path, *params_np)\n",
    "\n",
    "        if not results: return aggregated_parameters, {}\n",
    "        num_examples_total = sum(r.num_examples for _, r in results)\n",
    "        avg_loss = sum(r.metrics[\"train_loss\"] * r.num_examples for _, r in results) / num_examples_total\n",
    "        avg_acc = sum(r.metrics[\"train_accuracy\"] * r.num_examples for _, r in results) / num_examples_total\n",
    "        avg_prec = sum(r.metrics[\"train_precision\"] * r.num_examples for _, r in results) / num_examples_total\n",
    "        print(f\"üìä Round {server_round} Aggregated Client Training -> Avg Loss: {avg_loss:.4f}, Avg Acc: {avg_acc:.4f}, Avg Prec: {avg_prec:.4f}\")\n",
    "        \n",
    "        # Add to centralized metrics\n",
    "        aggregated_metrics.update({\n",
    "            \"avg_train_loss\": avg_loss, \n",
    "            \"avg_train_accuracy\": avg_acc, \n",
    "            \"avg_train_precision\": avg_prec\n",
    "        })\n",
    "        return aggregated_parameters, aggregated_metrics\n",
    "\n",
    "    def aggregate_evaluate(self, server_round, results, failures):\n",
    "        # Aggregate metrics as usual\n",
    "        loss, metrics = super().aggregate_evaluate(server_round, results, failures)\n",
    "        \n",
    "        # Save the best model\n",
    "        if metrics and 'accuracy' in metrics:\n",
    "            val_acc = metrics['accuracy']\n",
    "            if val_acc > self.best_val_acc:\n",
    "                self.best_val_acc = val_acc\n",
    "                print(f\"üéâ Round {server_round}: New best validation accuracy: {self.best_val_acc:.4f}. Saving model...\")\n",
    "                # Get the *current* global model parameters\n",
    "                self.best_model_params = self.get_parameters(config={})\n",
    "                \n",
    "        return loss, metrics\n",
    "\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, client_id, config):\n",
    "        self.client_id = client_id\n",
    "        self.trainer = Trainer(config, client_id=self.client_id)\n",
    "    def get_parameters(self, config):\n",
    "        return [val.cpu().numpy() for _, val in self.trainer.model.state_dict().items()]\n",
    "    def set_parameters(self, parameters):\n",
    "        params_dict = zip(self.trainer.model.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "        self.trainer.model.load_state_dict(state_dict, strict=True)\n",
    "    def fit(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "        \n",
    "        # --- ü™≤ BUG FIX: Get server_round from config ---\n",
    "        current_round = config.get(\"server_round\", 1) \n",
    "        \n",
    "        loss, acc, prec = self.trainer.train_epoch(epoch=current_round)\n",
    "        \n",
    "        # --- ü™≤ BUG FIX: Use correct current_round in print ---\n",
    "        print(f\"üìà Client {self.client_id} Round {current_round} -> Loss: {loss:.4f}, Acc: {acc:.4f}, Prec: {prec:.4f}\")\n",
    "        \n",
    "        metrics = {\"train_loss\": loss, \"train_accuracy\": acc, \"train_precision\": prec}\n",
    "        return self.get_parameters(config={}), len(self.trainer.train_ds), metrics\n",
    "\n",
    "# --- Define Function to Create Clients ---\n",
    "def client_fn(cid: str) -> fl.client.Client:\n",
    "    client_config = {\n",
    "        'data': {\n",
    "            'train_csv': os.path.join(CLIENT_CSVS_PATH, f'client_{cid}_train.csv'),\n",
    "            'img_dir': os.path.join(REGENERATED_DATA_PATH, 'patches'),\n",
    "        }, 'model': {'num_classes': 4, 'type': 'resnet'},\n",
    "        'training': {\n",
    "            'experiment_name': FL_EXPERIMENT_NAME, 'outdir': FL_OUTDIR,\n",
    "            'epochs': 1, 'batch_size': 32, 'lr': 0.0001\n",
    "        }\n",
    "    }\n",
    "    return FlowerClient(client_id=int(cid), config=client_config).to_client()\n",
    "\n",
    "# --- Define Server Config for Global Validation ---\n",
    "server_config = {\n",
    "    'data': {\n",
    "        'val_csv': os.path.join(REGENERATED_DATA_PATH, 'csvs/val.csv'),\n",
    "        'test_csv': os.path.join(REGENERATED_DATA_PATH, 'csvs/test.csv'),\n",
    "        'img_dir': os.path.join(REGENERATED_DATA_PATH, 'patches'),\n",
    "    }, 'model': {'num_classes': 4, 'type': 'resnet'},\n",
    "    'training': { 'batch_size': 32, 'outdir': '/tmp/server_eval', 'epochs': 0 }\n",
    "}\n",
    "\n",
    "# --- ü™≤ BUG FIX: Add on_fit_config_fn to pass round number to client ---\n",
    "def fit_config(server_round: int):\n",
    "    config = {\n",
    "        \"server_round\": server_round,\n",
    "    }\n",
    "    return config\n",
    "\n",
    "strategy = FedAvgWithCheckpointing(\n",
    "    fraction_fit=1.0, \n",
    "    min_available_clients=3,\n",
    "    evaluate_fn=get_evaluate_fn(server_config),\n",
    "    on_fit_config_fn=fit_config, # <-- BUG FIX\n",
    "    checkpoint_dir=FL_CHECKPOINT_DIR # <-- NEW\n",
    ")\n",
    "\n",
    "# --- Run the Simulation ---\n",
    "NUM_ROUNDS = 5 # You can increase this\n",
    "NUM_CLIENTS = 3\n",
    "\n",
    "print(f\"üöÄ Starting federated simulation for {NUM_ROUNDS} rounds with {NUM_CLIENTS} clients...\")\n",
    "history = fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=NUM_ROUNDS),\n",
    "    strategy=strategy,\n",
    ")\n",
    "print(\"üèÅ Federated simulation finished.\")\n",
    "\n",
    "# --- Plot & Save ---\n",
    "if 'accuracy' in history.metrics_distributed:\n",
    "    val_accs = [metric[1]['accuracy'] for metric in history.metrics_distributed['accuracy']]\n",
    "    val_precs = [metric[1]['precision'] for metric in history.metrics_distributed['precision']]\n",
    "    rounds = [metric[0] for metric in history.metrics_distributed['accuracy']]\n",
    "    \n",
    "    # Check if 'avg_train_accuracy' key exists\n",
    "    if 'avg_train_accuracy' in history.metrics_centralized:\n",
    "        avg_train_accs = [metric[1] for metric in history.metrics_centralized['avg_train_accuracy']]\n",
    "    else:\n",
    "        print(\"Warning: 'avg_train_accuracy' not found in centralized metrics. Plotting validation only.\")\n",
    "        avg_train_accs = None\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    fig.suptitle('Federated Learning Performance', fontsize=16)\n",
    "    \n",
    "    axs[0].plot(rounds, val_accs, marker='o', label='Global Validation Accuracy')\n",
    "    if avg_train_accs:\n",
    "        axs[0].plot(rounds, avg_train_accs, marker='x', linestyle='--', label='Avg. Client Training Accuracy')\n",
    "    \n",
    "    axs[0].set_title('Accuracy over Rounds'); axs[0].set_xlabel('Round'); axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].grid(True); axs[0].legend()\n",
    "    \n",
    "    axs[1].plot(rounds, val_precs, marker='o', color='orange', label='Global Validation Precision')\n",
    "    axs[1].set_title('Global Model Validation Precision'); axs[1].set_xlabel('Round'); axs[1].set_ylabel('Precision')\n",
    "    axs[1].grid(True); axs[1].legend()\n",
    "    \n",
    "    plot_path = os.path.join(FL_OUTDIR, 'fl_training_plot.png')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96]); plt.savefig(plot_path); plt.show()\n",
    "    print(f\"üìà Saved FL training plot to {plot_path}\")\n",
    "    print(f\"\\nüéØ Final Validation Accuracy after {NUM_ROUNDS} rounds: {val_accs[-1]:.4f}\")\n",
    "else:\n",
    "    print(\"No validation accuracy metrics found to plot.\")\n",
    "\n",
    "# --- Save BEST Model ---\n",
    "print(\"\\n--- Saving Best Performing Global Model ---\")\n",
    "best_global_params = strategy.best_model_params\n",
    "if best_global_params:\n",
    "    final_model = load_model(server_config)\n",
    "    params_dict = zip(final_model.state_dict().keys(), fl.common.parameters_to_ndarrays(best_global_params))\n",
    "    state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "    final_model.load_state_dict(state_dict, strict=True)\n",
    "    \n",
    "    save_path = os.path.join(FL_OUTDIR, 'best_resnet.pt')\n",
    "    torch.save(final_model.state_dict(), save_path)\n",
    "    print(f\"‚úÖ Best global federated model (Val Acc: {strategy.best_val_acc:.4f}) saved to {save_path}\")\n",
    "    \n",
    "    # --- NEW: Run Final Test on Best FL Model ---\n",
    "    print(\"\\n--- Running Final Test on Best FL Model ---\")\n",
    "    \n",
    "    # Use the same 'server_config' to initialize a trainer with the test set\n",
    "    test_trainer = Trainer(server_config, client_id=\"Global_Test\")\n",
    "    test_trainer.model.load_state_dict(state_dict) # Load the best model state\n",
    "    \n",
    "    test_loss, test_acc, test_prec, all_labels, all_preds = test_trainer.validate(test_trainer.test_loader)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"üéØ FL MODEL - FINAL TEST RESULTS üéØ\")\n",
    "    print(f\"     Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test Precision: {test_prec:.4f}\")\n",
    "    print(\"=\"*30)\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=['Normal', 'Benign', 'In-situ', 'Invasive'], zero_division=0))\n",
    "else:\n",
    "    print(\"‚ùå No best model was saved (e.g., evaluation never ran).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 6. XAI ANALYSIS AND VISUALIZATION (UPDATED FOR GPU) ---\n",
    "from src.xai import XAIProcessor, overlay_heatmap\n",
    "import yaml\n",
    "\n",
    "# --- A. Setup for Analysis ---\n",
    "# FIX 1: Check for Mac 'mps' (GPU)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"--- XAI using device: {device} ---\")\n",
    "\n",
    "# --- Define Paths ---\n",
    "REGENERATED_DATA_DIR = 'data/processed'\n",
    "EXPERIMENTS_DIR = 'experiments'\n",
    "\n",
    "# Load a sample test image from the BACH dataset\n",
    "test_df = pd.read_csv(os.path.join(REGENERATED_DATA_DIR, 'csvs/test.csv'))\n",
    "try:\n",
    "    invasive_image_info = test_df[test_df['label'] == 3].iloc[0]\n",
    "    image_path = os.path.join(REGENERATED_DATA_DIR, 'patches', invasive_image_info['filename'])\n",
    "    original_image = Image.open(image_path).convert('RGB')\n",
    "except (IndexError, FileNotFoundError):\n",
    "    print(\"Could not find an 'invasive' test image. Using the first available image.\")\n",
    "    first_image_info = test_df.iloc[0]\n",
    "    image_path = os.path.join(REGENERATED_DATA_DIR, 'patches', first_image_info['filename'])\n",
    "    original_image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# --- B. Load All Models ---\n",
    "def load_trained_model(experiment_name):\n",
    "    # We create a dummy config just to load the model architecture\n",
    "    config = {'model': {'num_classes': 4, 'type': 'resnet'}}\n",
    "    model = load_model(config)\n",
    "    \n",
    "    # The important part is loading the saved weights\n",
    "    model_path = os.path.join(EXPERIMENTS_DIR, experiment_name, 'best_resnet.pt')\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Warning: Model weights not found at {model_path}. Using last_epoch.pt instead.\")\n",
    "        model_path = os.path.join(EXPERIMENTS_DIR, experiment_name, 'last_epoch.pt')\n",
    "        if not os.path.exists(model_path):\n",
    "             print(f\"Error: Could not find any model weights for {experiment_name}.\")\n",
    "             return None\n",
    "\n",
    "    # FIX 2: Make sure to load the model to the correct device\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device) # Move model to GPU\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "models_to_analyze = {\n",
    "    \"Centralized\": load_trained_model('centralized_baseline'),\n",
    "    \"Local Client 0 (Normal/Benign Bias)\": load_trained_model('local_only_client_0'),\n",
    "    \"Local Client 2 (Invasive Bias)\": load_trained_model('local_only_client_2'),\n",
    "    \"Federated (Global)\": load_trained_model('federated_run_final')\n",
    "}\n",
    "\n",
    "# Filter out any models that failed to load\n",
    "models_to_analyze = {name: model for name, model in models_to_analyze.items() if model is not None}\n",
    "\n",
    "\n",
    "# --- C. Generate and Plot Grad-CAMs ---\n",
    "if models_to_analyze:\n",
    "    fig, axes = plt.subplots(1, len(models_to_analyze) + 1, figsize=(20, 5))\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title(\"Original Invasive Image\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    plot_idx = 1\n",
    "    for model_name, model in models_to_analyze.items():\n",
    "        print(f\"Generating Grad-CAM for {model_name}...\")\n",
    "        xai_processor = XAIProcessor(model, device, model_type='resnet')\n",
    "        \n",
    "        # We generate the heatmap for the target class 'Invasive' (3)\n",
    "        heatmap = xai_processor.gradcam(original_image, target_class=3)\n",
    "        overlay = overlay_heatmap(original_image.resize((224, 224)), heatmap)\n",
    "        \n",
    "        axes[plot_idx].imshow(overlay)\n",
    "        axes[plot_idx].set_title(model_name)\n",
    "        axes[plot_idx].axis('off')\n",
    "        plot_idx += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"\\n‚úÖ XAI analysis complete. Compare the heatmaps to see what each model learned.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No models were loaded successfully. Cannot perform XAI analysis.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8129080,
     "sourceId": 12852501,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
